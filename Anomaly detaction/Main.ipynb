{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd41aca",
   "metadata": {},
   "source": [
    "# Anomaly Detection using VQVAE\n",
    "\n",
    "## Student : Hussin Almoustafa \n",
    "\n",
    "This notebook implements anomaly detection using a Vector Quantized Variational Autoencoder (VQVAE) on X-ray images. The implementation uses TensorFlow and the Keras API.\n",
    "\n",
    "The implementation consists of the following main steps:\n",
    "\n",
    "1. Preparing the data: The notebook loads two datasets of X-ray images (normal and unhealthy) and prepares them for training and validation.\n",
    "\n",
    "\n",
    "\n",
    "2. Defining the VQVAE model: The VQVAE model is defined using TensorFlow and the Keras API.\n",
    "\n",
    "\n",
    "\n",
    "3. Training the VQVAE model: The VQVAE model is trained on the normal X-ray dataset.\n",
    "\n",
    "\n",
    "\n",
    "4. Evaluating the VQVAE model: The trained VQVAE model is evaluated on the validation set and the performance metrics (Mean Squared Error and Peak Signal to Noise Ratio) are printed.\n",
    "\n",
    "\n",
    "\n",
    "5. Anomaly detection: An AnomalyDetector class is defined that uses the trained VQVAE model to detect anomalies in the unhealthy X-ray dataset.\n",
    "\n",
    "\n",
    "\n",
    "6. Visualizing the anomaly detection results: The results of the anomaly detection on a specific X-ray image are visualized using a heatmap.\n",
    "\n",
    "# Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e4eff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VQVAE import ResidualBlock, VectorQuantizer, VQVAE\n",
    "from Anomaly_detaction import AnomalyDetector\n",
    "from Dataset import UnhealthyXRayDataset, NormalDataset\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf037662",
   "metadata": {},
   "source": [
    "# Define hyperparameters\n",
    "Next, we define the hyperparameters for our VQ-VAE model, including the batch size, number of epochs, learning rate, number of embeddings, embedding dimensions, and commitment cost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7ffbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "initial_learning_rate = 0.001\n",
    "num_embeddings = 64\n",
    "embedding_dim = 64\n",
    "commitment_cost = 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0972b78",
   "metadata": {},
   "source": [
    "# Define the paths to the normal and unhealthy X-ray image directories\n",
    "\n",
    "We will be using the Chest X-Ray Images dataset from Kaggle, which contains images of chest X-rays for patients with and without pneumonia. First, we define the paths for the normal and unhealthy image directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442b65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_image_path = \"C:/Users/hasso/Desktop/Hu/jaar 3/INNO/Image-Processing-and-Computer-Vision/Anomaly detaction/chest_xray/chest_xray/train/NORMAL/\"  # List of normal image paths\n",
    "\n",
    "unhealthy_image_path = \"C:/Users/hasso/Desktop/Hu/jaar 3/INNO/Image-Processing-and-Computer-Vision/Anomaly detaction/chest_xray/chest_xray/train/PNEUMONIA/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e650a",
   "metadata": {},
   "source": [
    "# Create instances of the NormalDataset and UnhealthyXRayDataset\n",
    "\n",
    "We then create instances of the NormalDataset and UnhealthyXRayDataset classes, which are custom datasets that we defined in the Dataset.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be87bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the NormalDataset and UnhealthyXRayDataset\n",
    "normal_dataset_instance = NormalDataset(normal_image_path)\n",
    "unhealthy_dataset_instance = UnhealthyXRayDataset(unhealthy_image_path)\n",
    "\n",
    "# Create tf.data.Dataset objects for both datasets\n",
    "normal_dataset = tf.data.Dataset.from_generator(normal_dataset_instance.generator,\n",
    "                                                output_types=(tf.float32, tf.int32),\n",
    "                                                output_shapes=(\n",
    "                                                    tf.TensorShape([None, None, 1]),\n",
    "                                                    tf.TensorShape([]),\n",
    "                                                ))\n",
    "\n",
    "unhealthy_dataset = tf.data.Dataset.from_generator(unhealthy_dataset_instance.generator,\n",
    "                                                   output_types=(tf.float32, tf.int32),\n",
    "                                                   output_shapes=(\n",
    "                                                       tf.TensorShape([None, None, 1]),\n",
    "                                                       tf.TensorShape([]),\n",
    "                                                   ))\n",
    "\n",
    "# Split the normal dataset into train and validation sets\n",
    "train_size = int(0.8 * len(normal_dataset_instance))\n",
    "val_size = len(normal_dataset_instance) - train_size\n",
    "\n",
    "train_normal_dataset = normal_dataset.take(train_size)\n",
    "val_normal_dataset = normal_dataset.skip(train_size)\n",
    "\n",
    "# Create dataloaders for the normal and unhealthy datasets\n",
    "train_normal_dataloader = train_normal_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_normal_dataloader = val_normal_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "unhealthy_dataloader = unhealthy_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe7ff5",
   "metadata": {},
   "source": [
    "# Train the VQ-VAE Model\n",
    " we create an instance of the VQ-VAE model with the specified hyperparameters. The VQ-VAE model is implemented in the VQVAE.py file, which we import at the beginning of the notebook. We create an instance of the VQVAE class and pass in the required hyperparameters, such as the input image channel, number of embeddings, embedding dimension, and commitment cost.\n",
    " \n",
    " We also set the batch size, number of epochs, and learning rate as hyperparameters for training the model later. The in_channels parameter is set to 1 since our input images are grayscale. The num_embeddings and embedding_dim parameters control the number and size of the codebook vectors, respectively. Finally, the commitment_cost parameter controls the trade-off between reconstruction accuracy and codebook usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reate an instance of the VQVAE model\n",
    "vqvae = VQVAE(in_channels=1, num_embeddings=num_embeddings, embedding_dim=embedding_dim, commitment_cost=commitment_cost)\n",
    "\n",
    "dataset_size = len(normal_dataset_instance)\n",
    "# Train the model on the training dataset\n",
    "vqvae.train_vqvae(train_normal_dataloader, num_epochs, initial_learning_rate, dataset_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f22a4",
   "metadata": {},
   "source": [
    "#  Evaluate the performance of the trained VQ-VAE model.\n",
    "\n",
    "First, we calculate the validation loss on the validation dataset using the trained model. We iterate through the validation dataloader and compute the reconstruction loss and the VQ loss. We add these losses to obtain the total loss for each batch and then average the total loss across all batches to obtain the validation loss for the entire validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "num_val_batches = 0\n",
    "\n",
    "for x, _ in val_normal_dataloader:\n",
    "    x_recon, z_e, z_q, indices, vq_loss = vqvae(x, training=False)\n",
    "    recon_loss = tf.reduce_mean(tf.keras.losses.MSE(x_recon, x))\n",
    "    loss = recon_loss + vq_loss\n",
    "    total_loss += loss.numpy()\n",
    "    num_val_batches += 1\n",
    "\n",
    "avg_loss = total_loss / num_val_batches\n",
    "print(f'Validation loss: {avg_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294fc29",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae768abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "vqvae.save_model(\"modelf.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e61a3",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test dataset \n",
    "\n",
    "Next, we evaluate the model on the test dataset. We iterate through the validation dataloader and calculate the mean squared error (MSE) and the peak signal-to-noise ratio (PSNR) for the reconstructed images and the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test dataset\n",
    "mse = tf.keras.metrics.MeanSquaredError()\n",
    "psnr_values = []\n",
    "\n",
    "for x_test, _ in val_normal_dataloader:\n",
    "    x_test = tf.cast(x_test, tf.float32)\n",
    "    x_recon, _, _, _, _ = vqvae(x_test)\n",
    "    mse.update_state(x_test, x_recon)\n",
    "    psnr_value = tf.image.psnr(x_test, x_recon, max_val=1.0)\n",
    "    psnr_values.append(psnr_value)\n",
    "\n",
    "# Print MSE and PSNR scores\n",
    "psnr_values = tf.concat(psnr_values, axis=0)  # Concatenate PSNR values before taking mean\n",
    "print(f\"Mean Squared Error: {mse.result().numpy():.4f}\")\n",
    "print(f\"PSNR: {tf.reduce_mean(psnr_values).numpy():.4f}\")\n",
    "\n",
    "# Plot reconstructed images and original images side by side\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20, 4))\n",
    "\n",
    "for i, (x_test, _) in enumerate(val_normal_dataloader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "\n",
    "    x_test = tf.cast(x_test, tf.float32)\n",
    "    x_recon, _, _, _, _ = vqvae(x_test)\n",
    "    x_test = np.squeeze(x_test.numpy())\n",
    "    x_recon = np.squeeze(x_recon.numpy())\n",
    "\n",
    "    axes[0][i].imshow(x_test[0], cmap='gray')\n",
    "    axes[1][i].imshow(x_recon[0], cmap='gray')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600dd948",
   "metadata": {},
   "source": [
    "# Apply anomaly detection on the unhealthy dataset\n",
    "\n",
    "Finally, we apply anomaly detection on the unhealthy dataset using the trained model and the anomaly detector. We detect anomalies in the dataset and visualize the anomaly detection results for a specific sample.\n",
    "\n",
    "Note: The anomaly detection part assumes that we have already instantiated and trained an AnomalyDetector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AnomalyDetector class\n",
    "anomaly_detector = AnomalyDetector(vqvae, threshold=0.05)\n",
    "# Apply anomaly detection on the unhealthy dataset\n",
    "anomalies = anomaly_detector.detect_anomalies(unhealthy_dataloader)\n",
    "print(f\"Anomalies detected: {len(anomalies)}\")\n",
    "\n",
    "# Find the sample in the unhealthy_dataloader\n",
    "sample_index = 0  # Choose the index of the sample you want to visualize\n",
    "sample_x = None\n",
    "\n",
    "for idx, (x, _) in enumerate(unhealthy_dataloader):\n",
    "    if idx == sample_index:\n",
    "        sample_x = x\n",
    "        break\n",
    "\n",
    "if sample_x is None:\n",
    "    raise ValueError(f\"Sample index {sample_index} is out of range.\")\n",
    "\n",
    "# Visualize the anomaly detection results for a specific sample\n",
    "x_recon, _, _, _, _ = vqvae(sample_x[np.newaxis, ...])\n",
    "anomaly_detector.visualize_heatmap(sample_x, x_recon[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a318b7b",
   "metadata": {},
   "source": [
    "Anomalie detectie is een techniek die wordt gebruikt om zeldzame gebeurtenissen of items te identificeren die aanzienlijk afwijken van het grootste deel van de data. Deze afwijkende items worden anomalieën of uitbijters genoemd en kunnen duiden op kritieke incidenten, zoals fraude, storingen, defecten of andere ongewenste gebeurtenissen. In de context van beeldverwerking kan anomalie detectie helpen bij het identificeren van ongebruikelijke of onverwachte patronen in afbeeldingen die mogelijk verband houden met defecten, beschadigingen of andere ongewenste kenmerken.\n",
    "\n",
    "VQ-VAE (Vector Quantized Variational Autoencoder) is een generatief model dat leert om complexe gegevens, zoals afbeeldingen, te comprimeren en te reconstrueren. In tegenstelling tot traditionele autoencoders, gebruikt VQ-VAE een discrete latente ruimte door middel van vector kwantisatie, wat resulteert in een betere compressie en een efficiëntere generatie van nieuwe gegevens. Het VQ-VAE model bestaat uit een encoder, die de invoerdata comprimeert in een latente ruimte, en een decoder, die de latente representatie terug omzet naar de oorspronkelijke vorm van de gegevens. Tijdens het trainingsproces optimaliseert het model de volgende objectieve functie:\n",
    "\n",
    "\\begin{equation}\n",
    "L(x) = \\mathbb{E}_{q(z|x)}[-\\log p(x|z)] + \\beta KL(q(z|x)||p(z))\n",
    "\\end{equation}\n",
    "\n",
    "Waarbij $L(x)$ de verliesfunctie is, $x$ de invoerdata, $z$ de latente variabele, $p(x|z)$ de decoder, $q(z|x)$ de encoder, en $p(z)$ de prior distributie van de latente variabelen. De term $\\beta$ is een hyperparameter die de afweging tussen de reconstructie- en regularisatieterm regelt.\n",
    "\n",
    "Anomalie detectie met VQ-VAE maakt gebruik van de reconstructie-afwijking tussen de oorspronkelijke afbeelding en de gereconstrueerde afbeelding om anomalieën te identificeren. Een hoge reconstructie-afwijking duidt op een afbeelding die sterk afwijkt van wat het model heeft geleerd, wat suggereert dat het een anomalie kan zijn.\n",
    "\n",
    "Het is belangrijk op te merken dat het trainen van VQ-VAE modellen veel rekenkracht en data vereist. Om effectief te zijn in het leren van de onderliggende structuren en patronen in de gegevens, heeft het model toegang nodig tot een grote hoeveelheid voorbeelden. Bovendien vereist de training van het model geavanceerde hardware, zoals grafische verwerkingseenheden (GPU's), om de benodigde berekeningen efficiënt uit te voeren. Dit kan de kosten en de tijd die nodig is om een effectief anomalie detectie systeem te ontwikkelen aanzienlijk verhogen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138add1",
   "metadata": {},
   "source": [
    "# Conclusie \n",
    "\n",
    "Anomalie detectie met VQ-VAE is een veelbelovende techniek voor het identificeren van ongebruikelijke of onverwachte patronen in afbeeldingen. De kracht van VQ-VAE ligt in het vermogen om complexe gegevens efficiënt te comprimeren en te reconstrueren, waardoor het model in staat is om subtiele verschillen tussen normale en afwijkende patronen vast te leggen.\n",
    "\n",
    "Echter, het trainen van een VQ-VAE model voor anomalie detectie is een rekenkundig intensief proces dat veel data en geavanceerde hardware vereist. Dit kan leiden tot hoge kosten en langere ontwikkeltijden, wat een belemmering kan vormen voor sommige toepassingen.\n",
    "\n",
    "Ondanks deze uitdagingen biedt VQ-VAE een effectieve benadering voor anomalie detectie in beeldverwerking en computer vision. Door voortdurend onderzoek en verbeteringen in de algoritmen en hardware, is het waarschijnlijk dat deze techniek verder zal worden geoptimaliseerd en toegepast in een breed scala van industrieën en toepassingen, variërend van productinspectie tot medische beeldanalyse en beveiliging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
